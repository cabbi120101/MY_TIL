{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (2.17.0)\n",
      "Requirement already satisfied: numpy in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.65.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.4.1)\n",
      "Requirement already satisfied: packaging in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: namex in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: rich in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: optree in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard<2.18,>=2.17->tensorflow) (7.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.18,>=2.17->tensorflow) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.2.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow-gpu\n",
      "  Downloading tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
      "Collecting python_version>\"3.7\"\n",
      "  Downloading python_version-0.0.2-py2.py3-none-any.whl (3.4 kB)\n",
      "Building wheels for collected packages: tensorflow-gpu\n",
      "  Building wheel for tensorflow-gpu (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /Library/Developer/CommandLineTools/usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/nd/hs1yql8s5k9bdqkwfv8sdngh0000gn/T/pip-install-bgzyodw0/tensorflow-gpu_323a8121775c42ceacbabd93db6300a4/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/nd/hs1yql8s5k9bdqkwfv8sdngh0000gn/T/pip-install-bgzyodw0/tensorflow-gpu_323a8121775c42ceacbabd93db6300a4/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /private/var/folders/nd/hs1yql8s5k9bdqkwfv8sdngh0000gn/T/pip-wheel-y7hfvvoc\n",
      "       cwd: /private/var/folders/nd/hs1yql8s5k9bdqkwfv8sdngh0000gn/T/pip-install-bgzyodw0/tensorflow-gpu_323a8121775c42ceacbabd93db6300a4/\n",
      "  Complete output (17 lines):\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"/private/var/folders/nd/hs1yql8s5k9bdqkwfv8sdngh0000gn/T/pip-install-bgzyodw0/tensorflow-gpu_323a8121775c42ceacbabd93db6300a4/setup.py\", line 37, in <module>\n",
      "      raise Exception(TF_REMOVAL_WARNING)\n",
      "  Exception:\n",
      "  \n",
      "  =========================================================\n",
      "  The \"tensorflow-gpu\" package has been removed!\n",
      "  \n",
      "  Please install \"tensorflow\" instead.\n",
      "  \n",
      "  Other than the name, the two packages have been identical\n",
      "  since TensorFlow 2.1, or roughly since Sep 2019. For more\n",
      "  information, see: pypi.org/project/tensorflow-gpu\n",
      "  =========================================================\n",
      "  \n",
      "  \n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for tensorflow-gpu\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for tensorflow-gpu\n",
      "Failed to build tensorflow-gpu\n",
      "Installing collected packages: python-version, tensorflow-gpu\n",
      "    Running setup.py install for tensorflow-gpu ... \u001b[?25lerror\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /Library/Developer/CommandLineTools/usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/nd/hs1yql8s5k9bdqkwfv8sdngh0000gn/T/pip-install-bgzyodw0/tensorflow-gpu_323a8121775c42ceacbabd93db6300a4/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/nd/hs1yql8s5k9bdqkwfv8sdngh0000gn/T/pip-install-bgzyodw0/tensorflow-gpu_323a8121775c42ceacbabd93db6300a4/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/nd/hs1yql8s5k9bdqkwfv8sdngh0000gn/T/pip-record-98tpjxab/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /Users/dongwook/Library/Python/3.9/include/python3.9/tensorflow-gpu\n",
      "         cwd: /private/var/folders/nd/hs1yql8s5k9bdqkwfv8sdngh0000gn/T/pip-install-bgzyodw0/tensorflow-gpu_323a8121775c42ceacbabd93db6300a4/\n",
      "    Complete output (17 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/private/var/folders/nd/hs1yql8s5k9bdqkwfv8sdngh0000gn/T/pip-install-bgzyodw0/tensorflow-gpu_323a8121775c42ceacbabd93db6300a4/setup.py\", line 37, in <module>\n",
      "        raise Exception(TF_REMOVAL_WARNING)\n",
      "    Exception:\n",
      "    \n",
      "    =========================================================\n",
      "    The \"tensorflow-gpu\" package has been removed!\n",
      "    \n",
      "    Please install \"tensorflow\" instead.\n",
      "    \n",
      "    Other than the name, the two packages have been identical\n",
      "    since TensorFlow 2.1, or roughly since Sep 2019. For more\n",
      "    information, see: pypi.org/project/tensorflow-gpu\n",
      "    =========================================================\n",
      "    \n",
      "    \n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Command errored out with exit status 1: /Library/Developer/CommandLineTools/usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/nd/hs1yql8s5k9bdqkwfv8sdngh0000gn/T/pip-install-bgzyodw0/tensorflow-gpu_323a8121775c42ceacbabd93db6300a4/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/nd/hs1yql8s5k9bdqkwfv8sdngh0000gn/T/pip-install-bgzyodw0/tensorflow-gpu_323a8121775c42ceacbabd93db6300a4/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/nd/hs1yql8s5k9bdqkwfv8sdngh0000gn/T/pip-record-98tpjxab/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /Users/dongwook/Library/Python/3.9/include/python3.9/tensorflow-gpu Check the logs for full command output.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 8.0 MB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.4\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[K     |████████████████████████████████| 721 kB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25h\u001b[33m  WARNING: Value for prefixed-purelib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/nd/hs1yql8s5k9bdqkwfv8sdngh0000gn/T/pip-build-env-wp1puhl6/normal/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/nd/hs1yql8s5k9bdqkwfv8sdngh0000gn/T/pip-build-env-wp1puhl6/normal/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Additional context:\n",
      "  user = False\n",
      "  home = None\n",
      "  root = None\n",
      "  prefix = '/private/var/folders/nd/hs1yql8s5k9bdqkwfv8sdngh0000gn/T/pip-build-env-wp1puhl6/normal'\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-purelib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/nd/hs1yql8s5k9bdqkwfv8sdngh0000gn/T/pip-build-env-wp1puhl6/overlay/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/nd/hs1yql8s5k9bdqkwfv8sdngh0000gn/T/pip-build-env-wp1puhl6/overlay/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Additional context:\n",
      "  user = False\n",
      "  home = None\n",
      "  root = None\n",
      "  prefix = '/private/var/folders/nd/hs1yql8s5k9bdqkwfv8sdngh0000gn/T/pip-build-env-wp1puhl6/overlay'\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gym-notices>=0.0.4\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from gym) (7.0.1)\n",
      "Collecting cloudpickle>=1.2.0\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/dongwook/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.8.0->gym) (3.17.0)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827655 sha256=528701a90da27558fc4912e810001fe70cd06832ffb37943c32eb37a8cb52709\n",
      "  Stored in directory: /Users/dongwook/Library/Caches/pip/wheels/af/2b/30/5e78b8b9599f2a2286a582b8da80594f654bf0e18d825a4405\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, cloudpickle, gym\n",
      "Successfully installed cloudpickle-3.0.0 gym-0.26.2 gym-notices-0.0.8\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용 여부 확인\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2단계: 환경 정의\n",
    "강화학습에서 사용할 배달 환경을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeliveryEnv(gym.Env):\n",
    "    def __init__(self, orders, riders, dist_mat):\n",
    "        super(DeliveryEnv, self).__init__()\n",
    "        self.orders = orders\n",
    "        self.riders = riders\n",
    "        self.dist_mat = dist_mat\n",
    "        self.K = len(orders)\n",
    "\n",
    "        # 상태 공간 정의: 주문의 상태 (0: 미처리, 1: 픽업 완료, 2: 배달 완료)\n",
    "        self.observation_space = spaces.Box(low=0, high=2, shape=(self.K,), dtype=np.int32)\n",
    "\n",
    "        # 행동 공간 정의: 각 주문의 픽업 또는 배달\n",
    "        self.action_space = spaces.Discrete(2 * self.K)\n",
    "\n",
    "        # 초기 상태 설정\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.zeros(self.K, dtype=np.int32)\n",
    "        self.current_time = 0  # 현재 시간 초기화\n",
    "        return self.state.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        order_id = action % self.K\n",
    "        is_pickup = action < self.K\n",
    "\n",
    "        # 보상 및 종료 조건 초기화\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        if is_pickup:\n",
    "            if self.state[order_id] == 0 and self.current_time >= self.orders[order_id][1] + self.orders[order_id][6]:\n",
    "                self.state[order_id] = 1\n",
    "                reward += 1  # 픽업 성공 보상\n",
    "        else:\n",
    "            if self.state[order_id] == 1:\n",
    "                delivery_time = self.current_time + self.get_travel_time(order_id, is_pickup=False)\n",
    "                if delivery_time <= self.orders[order_id][8]:\n",
    "                    self.state[order_id] = 2\n",
    "                    reward += 10  # 배달 성공 보상\n",
    "                self.current_time = delivery_time  # 현재 시간을 배달 시간으로 갱신\n",
    "\n",
    "        # 모든 주문이 배달 완료되었는지 확인\n",
    "        if np.all(self.state == 2):\n",
    "            done = True\n",
    "\n",
    "        return self.state.astype(np.float32), reward, done, {}\n",
    "\n",
    "    def get_travel_time(self, order_id, is_pickup):\n",
    "        if is_pickup:\n",
    "            return 0  # 픽업은 즉시 완료\n",
    "        else:\n",
    "            # 배달 위치까지의 이동 시간 계산\n",
    "            return int(np.round(self.dist_mat[order_id][order_id + self.K] / self.riders[0][1]))\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3단계: PPO 알고리즘 정의\n",
    "PPO 알고리즘을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, env, gamma=0.99, lr=0.001, clip_ratio=0.2, update_steps=10):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.update_steps = update_steps\n",
    "\n",
    "        # 배우 및 비평가 모델 정의\n",
    "        self.actor_model = self.build_actor_model()\n",
    "        self.critic_model = self.build_critic_model()\n",
    "\n",
    "        self.actor_optimizer = Adam(learning_rate=self.lr)\n",
    "        self.critic_optimizer = Adam(learning_rate=self.lr)\n",
    "\n",
    "    def build_actor_model(self):\n",
    "        model = Sequential([\n",
    "            Input(shape=(self.env.observation_space.shape[0],)),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(self.env.action_space.n, activation='softmax')\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    def build_critic_model(self):\n",
    "        model = Sequential([\n",
    "            Input(shape=(self.env.observation_space.shape[0],)),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = np.expand_dims(state, axis=0)  # 상태를 (1, K) 형태로 확장\n",
    "        state = tf.convert_to_tensor(state, dtype=tf.float32)  # 상태를 float32로 변환\n",
    "        action_probs = self.actor_model(state)\n",
    "        action = np.random.choice(self.env.action_space.n, p=action_probs.numpy().flatten())\n",
    "        return action\n",
    "\n",
    "    def learn(self, states, actions, rewards, next_states, dones):\n",
    "        # Advantage 계산\n",
    "        states = tf.convert_to_tensor(np.vstack(states), dtype=tf.float32)\n",
    "        next_states = tf.convert_to_tensor(np.vstack(next_states), dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(np.array(actions), dtype=tf.int32)\n",
    "        rewards = tf.convert_to_tensor(np.array(rewards), dtype=tf.float32)\n",
    "        dones = tf.convert_to_tensor(np.array(dones), dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            values = tf.squeeze(self.critic_model(states))\n",
    "            next_values = tf.squeeze(self.critic_model(next_states))\n",
    "            targets = rewards + self.gamma * next_values * (1 - dones)\n",
    "            advantages = targets - values\n",
    "            value_loss = tf.reduce_mean(tf.square(targets - values))\n",
    "\n",
    "        grads = tape.gradient(value_loss, self.critic_model.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(zip(grads, self.critic_model.trainable_variables))\n",
    "\n",
    "        # Actor 업데이트\n",
    "        with tf.GradientTape() as tape:\n",
    "            action_probs = self.actor_model(states)\n",
    "            action_log_probs = tf.reduce_sum(tf.one_hot(actions, self.env.action_space.n) * tf.math.log(action_probs), axis=1)\n",
    "            old_action_log_probs = tf.stop_gradient(action_log_probs)\n",
    "            ratios = tf.exp(action_log_probs - old_action_log_probs)\n",
    "            clipped_ratios = tf.clip_by_value(ratios, 1 - self.clip_ratio, 1 + self.clip_ratio)\n",
    "            actor_loss = -tf.reduce_mean(tf.minimum(ratios * advantages, clipped_ratios * advantages))\n",
    "\n",
    "        grads = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(zip(grads, self.actor_model.trainable_variables))\n",
    "\n",
    "    def train(self, episodes=200, max_steps=200):\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "\n",
    "            for step in range(max_steps):\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                next_states.append(next_state)\n",
    "                dones.append(done)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.learn(states, actions, rewards, next_states, dones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24 environments.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "data_dir = 'test_data'\n",
    "data_files = [os.path.join(data_dir, file) for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "\n",
    "environments = []\n",
    "for data_file in data_files:\n",
    "    with open(data_file, 'r') as f:\n",
    "        problem_data = json.load(f)\n",
    "    K = problem_data['K']\n",
    "    orders = problem_data['ORDERS']\n",
    "    riders = problem_data['RIDERS']\n",
    "    dist_mat = np.array(problem_data['DIST'])\n",
    "    env = DeliveryEnv(orders, riders, dist_mat)\n",
    "    environments.append(env)\n",
    "\n",
    "print(f\"Loaded {len(environments)} environments.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5단계: 모델 학습\n",
    "각 환경에 대해 모델을 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training on Environments: 100%|██████████| 24/24 [1:26:14<00:00, 215.59s/it]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "class DeliveryEnv(gym.Env):\n",
    "    def __init__(self, orders, riders, dist_mat, max_state_size):\n",
    "        super(DeliveryEnv, self).__init__()\n",
    "        self.orders = orders\n",
    "        self.riders = riders\n",
    "        self.dist_mat = dist_mat\n",
    "        self.K = len(orders)\n",
    "        self.n_riders = len(riders)\n",
    "\n",
    "        # 상태 공간 정의: 주문의 상태 (0: 미처리, 1: 픽업 완료, 2: 배달 완료)\n",
    "        self.observation_space = spaces.Box(low=0, high=2, shape=(max_state_size,), dtype=np.float32)\n",
    "\n",
    "        # 행동 공간 정의: 각 주문의 픽업 또는 배달\n",
    "        self.action_space = spaces.Discrete(max_state_size)\n",
    "\n",
    "        # 초기 상태 설정\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.zeros(self.observation_space.shape[0], dtype=np.float32)\n",
    "        self.current_time = 0  # 현재 시간 초기화\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        order_id = action % self.K\n",
    "        is_pickup = action < self.K\n",
    "\n",
    "        # 보상 및 종료 조건 초기화\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        if is_pickup:\n",
    "            if self.state[order_id] == 0 and self.current_time >= self.orders[order_id][1] + self.orders[order_id][6]:\n",
    "                self.state[order_id] = 1\n",
    "                reward += 1  # 픽업 성공 보상\n",
    "        else:\n",
    "            if self.state[order_id] == 1:\n",
    "                travel_time = self.get_travel_time(order_id, is_pickup=False)\n",
    "                delivery_time = self.current_time + travel_time\n",
    "                if delivery_time <= self.orders[order_id][8]:\n",
    "                    self.state[order_id] = 2\n",
    "                    reward += 10  # 배달 성공 보상\n",
    "                self.current_time = delivery_time  # 현재 시간을 배달 시간으로 갱신\n",
    "\n",
    "        # 모든 주문이 배달 완료되었는지 확인\n",
    "        if np.all(self.state[:self.K] == 2):\n",
    "            done = True\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def get_travel_time(self, order_id, is_pickup):\n",
    "        if is_pickup:\n",
    "            return 0  # 픽업은 즉시 완료\n",
    "        else:\n",
    "            # 배달 위치까지의 이동 시간 계산\n",
    "            return int(np.round(self.dist_mat[order_id][order_id + self.K] / self.riders[0][1]))\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env, state_size, gamma=0.99, lr=0.001, clip_ratio=0.2, update_steps=10):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.update_steps = update_steps\n",
    "\n",
    "        # 배우 및 비평가 모델 정의\n",
    "        self.actor_model = self.build_actor_model(state_size)\n",
    "        self.critic_model = self.build_critic_model(state_size)\n",
    "\n",
    "        self.actor_optimizer = Adam(learning_rate=self.lr)\n",
    "        self.critic_optimizer = Adam(learning_rate=self.lr)\n",
    "\n",
    "    def build_actor_model(self, state_size):\n",
    "        model = Sequential([\n",
    "            Input(shape=(state_size,)),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(self.env.action_space.n, activation='softmax')\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    def build_critic_model(self, state_size):\n",
    "        model = Sequential([\n",
    "            Input(shape=(state_size,)),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = np.expand_dims(state, axis=0)  # 상태를 (1, K) 형태로 확장\n",
    "        state = tf.convert_to_tensor(state, dtype=tf.float32)  # 상태를 float32로 변환\n",
    "        action_probs = self.actor_model(state, training=False).numpy().flatten()\n",
    "        assert len(action_probs) == self.env.action_space.n, \"action_probs와 action_space.n의 크기가 일치하지 않습니다.\"\n",
    "        action = np.random.choice(self.env.action_space.n, p=action_probs)\n",
    "        return action\n",
    "\n",
    "    def learn(self, states, actions, rewards, next_states, dones):\n",
    "        # Advantage 계산\n",
    "        states = tf.convert_to_tensor(np.vstack(states), dtype=tf.float32)\n",
    "        next_states = tf.convert_to_tensor(np.vstack(next_states), dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(np.array(actions), dtype=tf.int32)\n",
    "        rewards = tf.convert_to_tensor(np.array(rewards), dtype=tf.float32)\n",
    "        dones = tf.convert_to_tensor(np.array(dones), dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            values = tf.squeeze(self.critic_model(states, training=True))\n",
    "            next_values = tf.squeeze(self.critic_model(next_states, training=True))\n",
    "            targets = rewards + self.gamma * next_values * (1 - dones)\n",
    "            advantages = targets - values\n",
    "            value_loss = tf.reduce_mean(tf.square(targets - values))\n",
    "\n",
    "        grads = tape.gradient(value_loss, self.critic_model.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(zip(grads, self.critic_model.trainable_variables))\n",
    "\n",
    "        # Actor 업데이트\n",
    "        with tf.GradientTape() as tape:\n",
    "            action_probs = self.actor_model(states, training=True)\n",
    "            action_log_probs = tf.reduce_sum(tf.one_hot(actions, self.env.action_space.n) * tf.math.log(action_probs), axis=1)\n",
    "            old_action_log_probs = tf.stop_gradient(action_log_probs)\n",
    "            ratios = tf.exp(action_log_probs - old_action_log_probs)\n",
    "            clipped_ratios = tf.clip_by_value(ratios, 1 - self.clip_ratio, 1 + self.clip_ratio)\n",
    "            actor_loss = -tf.reduce_mean(tf.minimum(ratios * advantages, clipped_ratios * advantages))\n",
    "\n",
    "        grads = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(zip(grads, self.actor_model.trainable_variables))\n",
    "\n",
    "    def train(self, episodes=1000, max_steps=1000):\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "\n",
    "            for step in range(max_steps):\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                next_states.append(next_state)\n",
    "                dones.append(done)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.learn(states, actions, rewards, next_states, dones)\n",
    "\n",
    "def train_on_multiple_environments(agent, envs, data_files, episodes_per_env=200):\n",
    "    for env in tqdm(envs, desc=\"Training on Environments\"):\n",
    "        agent.env = env\n",
    "        agent.actor_model = agent.build_actor_model(agent.env.observation_space.shape[0])\n",
    "        agent.critic_model = agent.build_critic_model(agent.env.observation_space.shape[0])\n",
    "        agent.actor_optimizer = Adam(learning_rate=agent.lr)\n",
    "        agent.critic_optimizer = Adam(learning_rate=agent.lr)\n",
    "        agent.train(episodes=episodes_per_env)\n",
    "\n",
    "    agent.actor_model.save(\"actor_delivery_combined.keras\")\n",
    "    agent.critic_model.save(\"critic_delivery_combined.keras\")\n",
    "\n",
    "# 예제 실행\n",
    "data_dir = 'test_data'  # 데이터셋이 있는 디렉토리\n",
    "data_files = [os.path.join(data_dir, file) for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "\n",
    "environments = []\n",
    "max_state_size = 600\n",
    "\n",
    "for data_file in data_files:\n",
    "    with open(data_file, 'r') as f:\n",
    "        problem_data = json.load(f)\n",
    "    K = problem_data['K']\n",
    "    max_state_size = max(max_state_size, 2 * K)\n",
    "    orders = problem_data['ORDERS']\n",
    "    riders = problem_data['RIDERS']\n",
    "    dist_mat = np.array(problem_data['DIST'])\n",
    "    env = DeliveryEnv(orders, riders, dist_mat, max_state_size)\n",
    "    environments.append(env)\n",
    "\n",
    "ppo_agent = PPO(environments[0], max_state_size)  # 초기 환경을 하나 선택하여 에이전트 생성\n",
    "train_on_multiple_environments(ppo_agent, environments, data_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 확인 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "# 학습된 모델 로드\n",
    "actor_model = load_model('actor_delivery_combined.keras')\n",
    "critic_model = load_model('critic_delivery_combined.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 환경 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeliveryEnv:\n",
    "    def __init__(self, orders, riders, dist_mat):\n",
    "        self.orders = orders\n",
    "        self.riders = riders\n",
    "        self.dist_mat = dist_mat\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_position = [order[2:4] for order in self.orders]  # 시작 위치를 주문의 픽업 위치로 설정\n",
    "        self.current_time = [0 for _ in self.orders]  # 각 주문의 시작 시간을 0으로 설정\n",
    "        self.done = False\n",
    "        return self.current_state()\n",
    "\n",
    "    def current_state(self):\n",
    "        # 현재 상태를 반환하는 함수\n",
    "        state = np.concatenate([np.array(self.current_position).flatten(), np.array(self.current_time).flatten()])\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        # 액션을 수행하여 다음 상태, 보상, 완료 여부를 반환하는 함수\n",
    "        rider_idx, next_pos = divmod(action, len(self.orders))\n",
    "        dist = self.dist_mat[self.current_position[rider_idx], next_pos]\n",
    "        self.current_position[rider_idx] = self.orders[next_pos][4:6]\n",
    "        self.current_time[rider_idx] += dist / self.riders[rider_idx][1]\n",
    "        reward = -dist  # 거리만큼 보상을 마이너스로 설정 (거리가 짧을수록 보상이 큼)\n",
    "        done = all([time > order[7] for time, order in zip(self.current_time, self.orders)])\n",
    "        return self.current_state(), reward, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 평가 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(env, actor_model, critic_model):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        state = np.reshape(state, [1, len(state)])\n",
    "        action_probs = actor_model.predict(state, verbose=0)\n",
    "        action = np.argmax(action_probs[0])\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여러 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24 environments.\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'test_data'\n",
    "data_files = [os.path.join(data_dir, file) for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "\n",
    "environments = []\n",
    "for data_file in data_files:\n",
    "    with open(data_file, 'r') as f:\n",
    "        problem_data = json.load(f)\n",
    "    K = problem_data['K']\n",
    "    orders = problem_data['ORDERS']\n",
    "    riders = problem_data['RIDERS']\n",
    "    dist_mat = np.array(problem_data['DIST'])\n",
    "    env = DeliveryEnv(orders, riders, dist_mat)\n",
    "    environments.append(env)\n",
    "\n",
    "print(f\"Loaded {len(environments)} environments.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모든 환경에서 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[185], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m total_rewards \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m environments:\n\u001b[0;32m----> 3\u001b[0m     total_reward \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     total_rewards\u001b[38;5;241m.\u001b[39mappend(total_reward)\n",
      "Cell \u001b[0;32mIn[183], line 207\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(env, actor_model, critic_model)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    206\u001b[0m     state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(state, [\u001b[38;5;241m1\u001b[39m, env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m--> 207\u001b[0m     action_probs \u001b[38;5;241m=\u001b[39m \u001b[43mactor_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(action_probs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    209\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py:446\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28mself\u001b[39m, x, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    444\u001b[0m ):\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;66;03m# Create an iterator that yields batches of input data.\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     epoch_iterator \u001b[38;5;241m=\u001b[39m \u001b[43mTFEpochIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;66;03m# Container that configures and calls callbacks.\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py:629\u001b[0m, in \u001b[0;36mTFEpochIterator.__init__\u001b[0;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy \u001b[38;5;241m=\u001b[39m distribute_strategy\n\u001b[0;32m--> 629\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedDataset):\n\u001b[1;32m    631\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy\u001b[38;5;241m.\u001b[39mexperimental_distribute_dataset(\n\u001b[1;32m    632\u001b[0m         dataset\n\u001b[1;32m    633\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py:638\u001b[0m, in \u001b[0;36mTFEpochIterator._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_iterator\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/data_adapters/array_data_adapter.py:232\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mwith_options(options)\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[0;32m--> 232\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mindices_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_batch_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    234\u001b[0m     indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mmap(tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/data/ops/dataset_ops.py:2356\u001b[0m, in \u001b[0;36mDatasetV2.flat_map\u001b[0;34m(self, map_func, name)\u001b[0m\n\u001b[1;32m   2352\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> flat_map_op ->\u001b[39;00m\n\u001b[1;32m   2353\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   2354\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   2355\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flat_map_op\n\u001b[0;32m-> 2356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mflat_map_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/data/ops/flat_map_op.py:24\u001b[0m, in \u001b[0;36m_flat_map\u001b[0;34m(input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_flat_map\u001b[39m(input_dataset, map_func, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# pylint: disable=unused-private-name\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_FlatMapDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/data/ops/flat_map_op.py:33\u001b[0m, in \u001b[0;36m_FlatMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dataset, map_func, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dataset \u001b[38;5;241m=\u001b[39m input_dataset\n\u001b[0;32m---> 33\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39moutput_structure, dataset_ops\u001b[38;5;241m.\u001b[39mDatasetSpec):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `map_func` argument must return a `Dataset` object. Got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_ops\u001b[38;5;241m.\u001b[39mget_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39moutput_structure)\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/data/ops/structured_function.py:265\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    259\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    263\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1251\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1250\u001b[0m   \u001b[38;5;66;03m# Implements PolymorphicFunction.get_concrete_function.\u001b[39;00m\n\u001b[0;32m-> 1251\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m   concrete\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1253\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1221\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1220\u001b[0m     initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m   1225\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m   \u001b[38;5;66;03m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:696\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[1;32m    692\u001b[0m     variable_capturing_scope,\n\u001b[1;32m    693\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[1;32m    694\u001b[0m )\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[1;32m    701\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[1;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[0;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[1;32m    290\u001b[0m   )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[1;32m    304\u001b[0m       placeholder_context\n\u001b[1;32m    305\u001b[0m   )\n\u001b[1;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    309\u001b[0m )\n\u001b[0;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_control_dependencies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_acd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_type_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[1;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/framework/func_graph.py:1059\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1056\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m   1058\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1059\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:599\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    596\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    597\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 599\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/data/ops/structured_function.py:231\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):  \u001b[38;5;66;03m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m   ret \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_structure, ret)\n\u001b[1;32m    233\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m ret]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/data/ops/structured_function.py:161\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[1;32m    160\u001b[0m   nested_args \u001b[38;5;241m=\u001b[39m (nested_args,)\n\u001b[0;32m--> 161\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mautograph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_ctx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnested_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m ret \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(ret)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _should_pack(ret):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/autograph/impl/api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39muser_requested \u001b[38;5;129;01mand\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/autograph/impl/api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/data_adapters/array_data_adapter.py:160\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset.<locals>.slice_batch_indices\u001b[0;34m(indices)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert a Tensor of indices into a dataset of batched indices.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03mThis step can be accomplished in several ways. The most natural is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    A Dataset of batched indices.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    159\u001b[0m num_in_full_batch \u001b[38;5;241m=\u001b[39m num_full_batches \u001b[38;5;241m*\u001b[39m batch_size\n\u001b[0;32m--> 160\u001b[0m first_k_indices \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum_in_full_batch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m first_k_indices \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    162\u001b[0m     first_k_indices, [num_full_batches, batch_size]\n\u001b[1;32m    163\u001b[0m )\n\u001b[1;32m    165\u001b[0m flat_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(first_k_indices)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/ops/array_ops.py:990\u001b[0m, in \u001b[0;36mslice\u001b[0;34m(input_, begin, size, name)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslice\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mslice\u001b[39m(input_, begin, size, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    941\u001b[0m   \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Extracts a slice from a tensor.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m  See also `tf.strided_slice`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;124;03m    A `Tensor` the same type as `input_`.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 990\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/ops/gen_array_ops.py:9841\u001b[0m, in \u001b[0;36m_slice\u001b[0;34m(input, begin, size, name)\u001b[0m\n\u001b[1;32m   9839\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[1;32m   9840\u001b[0m \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[0;32m-> 9841\u001b[0m _, _, _op, _outputs \u001b[38;5;241m=\u001b[39m \u001b[43m_op_def_library\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_op_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9842\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSlice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbegin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   9843\u001b[0m _result \u001b[38;5;241m=\u001b[39m _outputs[:]\n\u001b[1;32m   9844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/framework/op_def_library.py:778\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m g\u001b[38;5;241m.\u001b[39mas_default(), ops\u001b[38;5;241m.\u001b[39mname_scope(name) \u001b[38;5;28;01mas\u001b[39;00m scope:\n\u001b[1;32m    777\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m fallback:\n\u001b[0;32m--> 778\u001b[0m     \u001b[43m_ExtractInputsAndAttrs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_type_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_list_attr_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_type_attr_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m                           \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m     _ExtractRemainingAttrs(op_type_name, op_def, keywords,\n\u001b[1;32m    782\u001b[0m                            default_type_attr_map, attrs)\n\u001b[1;32m    783\u001b[0m     _ExtractAttrProto(op_type_name, op_def, attrs, attr_protos)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/framework/op_def_library.py:594\u001b[0m, in \u001b[0;36m_ExtractInputsAndAttrs\u001b[0;34m(op_type_name, op_def, allowed_list_attr_map, keywords, default_type_attr_map, attrs, inputs, input_types)\u001b[0m\n\u001b[1;32m    587\u001b[0m             inferred_from[k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault in OpDef\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    589\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    590\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    591\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtypes\u001b[38;5;241m.\u001b[39mas_dtype(attrs[input_arg\u001b[38;5;241m.\u001b[39mtype_attr])\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    592\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minferred_from[input_arg\u001b[38;5;241m.\u001b[39mtype_attr]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 594\u001b[0m   types \u001b[38;5;241m=\u001b[39m [\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m]\n\u001b[1;32m    595\u001b[0m   inputs\u001b[38;5;241m.\u001b[39mappend(values)\n\u001b[1;32m    596\u001b[0m base_types \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mbase_dtype \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m types]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/framework/tensor.py:265\u001b[0m, in \u001b[0;36mTensor.dtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdtype\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    264\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"The `DType` of elements in this tensor.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_rewards = []\n",
    "for env in environments:\n",
    "    total_reward = evaluate_model(env, actor_model, critic_model)\n",
    "    total_rewards.append(total_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과 정리 및 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reward = np.mean(total_rewards)\n",
    "print(f\"평균 총 보상: {average_reward}\")\n",
    "print(f\"각 환경별 총 보상: {total_rewards}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training on Environments: 100%|██████████| 24/24 [38:58:32<00:00, 5846.34s/it]    \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_628\" is incompatible with the layer: expected axis -1 of input shape to have value 600, but received input with shape (1, 200)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 200), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[183], line 217\u001b[0m\n\u001b[1;32m    215\u001b[0m total_rewards \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m environments:\n\u001b[0;32m--> 217\u001b[0m     total_reward \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     total_rewards\u001b[38;5;241m.\u001b[39mappend(total_reward)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal rewards from each environment:\u001b[39m\u001b[38;5;124m\"\u001b[39m, total_rewards)\n",
      "Cell \u001b[0;32mIn[183], line 207\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(env, actor_model, critic_model)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    206\u001b[0m     state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(state, [\u001b[38;5;241m1\u001b[39m, env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m--> 207\u001b[0m     action_probs \u001b[38;5;241m=\u001b[39m \u001b[43mactor_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(action_probs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    209\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/layers/input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m    224\u001b[0m             value,\n\u001b[1;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    226\u001b[0m         }:\n\u001b[0;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m             )\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_628\" is incompatible with the layer: expected axis -1 of input shape to have value 600, but received input with shape (1, 200)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 200), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "class DeliveryEnv(gym.Env):\n",
    "    def __init__(self, orders, riders, dist_mat, max_state_size):\n",
    "        super(DeliveryEnv, self).__init__()\n",
    "        self.orders = orders\n",
    "        self.riders = riders\n",
    "        self.dist_mat = dist_mat\n",
    "        self.K = len(orders)\n",
    "        self.n_riders = len(riders)\n",
    "        self.max_state_size = max_state_size\n",
    "\n",
    "        # 상태 공간 정의: 주문의 상태 (0: 미처리, 1: 픽업 완료, 2: 배달 완료)\n",
    "        self.observation_space = spaces.Box(low=0, high=2, shape=(max_state_size,), dtype=np.float32)\n",
    "\n",
    "        # 행동 공간 정의: 각 주문의 픽업 또는 배달\n",
    "        self.action_space = spaces.Discrete(max_state_size)\n",
    "\n",
    "        # 초기 상태 설정\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.zeros(self.observation_space.shape[0], dtype=np.float32)\n",
    "        self.current_time = 0  # 현재 시간 초기화\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        order_id = action % self.K\n",
    "        is_pickup = action < self.K\n",
    "\n",
    "        # 보상 및 종료 조건 초기화\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        if is_pickup:\n",
    "            if self.state[order_id] == 0 and self.current_time >= self.orders[order_id][1] + self.orders[order_id][6]:\n",
    "                self.state[order_id] = 1\n",
    "                reward += 1  # 픽업 성공 보상\n",
    "        else:\n",
    "            if self.state[order_id] == 1:\n",
    "                travel_time = self.get_travel_time(order_id, is_pickup=False)\n",
    "                delivery_time = self.current_time + travel_time\n",
    "                if delivery_time <= self.orders[order_id][8]:\n",
    "                    self.state[order_id] = 2\n",
    "                    reward += 10  # 배달 성공 보상\n",
    "                self.current_time = delivery_time  # 현재 시간을 배달 시간으로 갱신\n",
    "\n",
    "        # 모든 주문이 배달 완료되었는지 확인\n",
    "        if np.all(self.state[:self.K] == 2):\n",
    "            done = True\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def get_travel_time(self, order_id, is_pickup):\n",
    "        if is_pickup:\n",
    "            return 0  # 픽업은 즉시 완료\n",
    "        else:\n",
    "            # 배달 위치까지의 이동 시간 계산\n",
    "            return int(np.round(self.dist_mat[order_id][order_id + self.K] / self.riders[0][1]))\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env, state_size, gamma=0.99, lr=0.001, clip_ratio=0.2, update_steps=10):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.update_steps = update_steps\n",
    "\n",
    "        # 배우 및 비평가 모델 정의\n",
    "        self.actor_model = self.build_actor_model(state_size)\n",
    "        self.critic_model = self.build_critic_model(state_size)\n",
    "\n",
    "        self.actor_optimizer = Adam(learning_rate=self.lr)\n",
    "        self.critic_optimizer = Adam(learning_rate=self.lr)\n",
    "\n",
    "    def build_actor_model(self, state_size):\n",
    "        model = Sequential([\n",
    "            Input(shape=(state_size,)),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(self.env.action_space.n, activation='softmax')\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    def build_critic_model(self, state_size):\n",
    "        model = Sequential([\n",
    "            Input(shape=(state_size,)),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = np.expand_dims(state, axis=0)  # 상태를 (1, K) 형태로 확장\n",
    "        state = tf.convert_to_tensor(state, dtype=tf.float32)  # 상태를 float32로 변환\n",
    "        action_probs = self.actor_model(state, training=False).numpy().flatten()\n",
    "        assert len(action_probs) == self.env.action_space.n, \"action_probs와 action_space.n의 크기가 일치하지 않습니다.\"\n",
    "        action = np.random.choice(self.env.action_space.n, p=action_probs)\n",
    "        return action\n",
    "\n",
    "    def learn(self, states, actions, rewards, next_states, dones):\n",
    "        # Advantage 계산\n",
    "        states = tf.convert_to_tensor(np.vstack(states), dtype=tf.float32)\n",
    "        next_states = tf.convert_to_tensor(np.vstack(next_states), dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(np.array(actions), dtype=tf.int32)\n",
    "        rewards = tf.convert_to_tensor(np.array(rewards), dtype=tf.float32)\n",
    "        dones = tf.convert_to_tensor(np.array(dones), dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            values = tf.squeeze(self.critic_model(states, training=True))\n",
    "            next_values = tf.squeeze(self.critic_model(next_states, training=True))\n",
    "            targets = rewards + self.gamma * next_values * (1 - dones)\n",
    "            advantages = targets - values\n",
    "            value_loss = tf.reduce_mean(tf.square(targets - values))\n",
    "\n",
    "        grads = tape.gradient(value_loss, self.critic_model.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(zip(grads, self.critic_model.trainable_variables))\n",
    "\n",
    "        # Actor 업데이트\n",
    "        with tf.GradientTape() as tape:\n",
    "            action_probs = self.actor_model(states, training=True)\n",
    "            action_log_probs = tf.reduce_sum(tf.one_hot(actions, self.env.action_space.n) * tf.math.log(action_probs), axis=1)\n",
    "            old_action_log_probs = tf.stop_gradient(action_log_probs)\n",
    "            ratios = tf.exp(action_log_probs - old_action_log_probs)\n",
    "            clipped_ratios = tf.clip_by_value(ratios, 1 - self.clip_ratio, 1 + self.clip_ratio)\n",
    "            actor_loss = -tf.reduce_mean(tf.minimum(ratios * advantages, clipped_ratios * advantages))\n",
    "\n",
    "        grads = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(zip(grads, self.actor_model.trainable_variables))\n",
    "\n",
    "    def train(self, episodes=1000, max_steps=1000):\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "\n",
    "            for step in range(max_steps):\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                next_states.append(next_state)\n",
    "                dones.append(done)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.learn(states, actions, rewards, next_states, dones)\n",
    "\n",
    "def train_on_multiple_environments(agent, envs, episodes_per_env=200):\n",
    "    for env in tqdm(envs, desc=\"Training on Environments\"):\n",
    "        agent.env = env\n",
    "        agent.actor_model = agent.build_actor_model(agent.env.observation_space.shape[0])\n",
    "        agent.critic_model = agent.build_critic_model(agent.env.observation_space.shape[0])\n",
    "        agent.actor_optimizer = Adam(learning_rate=agent.lr)\n",
    "        agent.critic_optimizer = Adam(learning_rate=agent.lr)\n",
    "        agent.train(episodes=episodes_per_env)\n",
    "\n",
    "    agent.actor_model.save(\"actor_delivery_combined.h5\")\n",
    "    agent.critic_model.save(\"critic_delivery_combined.h5\")\n",
    "\n",
    "# 예제 실행\n",
    "data_dir = 'test_data'  # 데이터셋이 있는 디렉토리\n",
    "data_files = [os.path.join(data_dir, file) for file in os.listdir(data_dir) if file.endswith('.json')]\n",
    "\n",
    "environments = []\n",
    "max_state_size = 0\n",
    "\n",
    "for data_file in data_files:\n",
    "    with open(data_file, 'r') as f:\n",
    "        problem_data = json.load(f)\n",
    "    K = problem_data['K']\n",
    "    max_state_size = max(max_state_size, 2 * K)\n",
    "    orders = problem_data['ORDERS']\n",
    "    riders = problem_data['RIDERS']\n",
    "    dist_mat = np.array(problem_data['DIST'])\n",
    "    env = DeliveryEnv(orders, riders, dist_mat, max_state_size)\n",
    "    environments.append(env)\n",
    "\n",
    "ppo_agent = PPO(environments[0], max_state_size)  # 초기 환경을 하나 선택하여 에이전트 생성\n",
    "train_on_multiple_environments(ppo_agent, environments)\n",
    "\n",
    "# 모델 평가\n",
    "actor_model = load_model(\"actor_delivery_combined.h5\")\n",
    "critic_model = load_model(\"critic_delivery_combined.h5\")\n",
    "\n",
    "def evaluate_model(env, actor_model, critic_model):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = np.reshape(state, [1, env.observation_space.shape[0]])\n",
    "        action_probs = actor_model.predict(state, verbose=0)\n",
    "        action = np.argmax(action_probs[0])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "    return total_reward\n",
    "\n",
    "# 모든 환경에서 평가\n",
    "total_rewards = []\n",
    "for env in environments:\n",
    "    total_reward = evaluate_model(env, actor_model, critic_model)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "print(\"Total rewards from each environment:\", total_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
