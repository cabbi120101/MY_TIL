### 1. **마르코프 결정 과정 (Markov Decision Process, MDP)**

강화학습 문제는 일반적으로 MDP로 모델링됩니다. MDP는 다음의 4가지 요소로 구성됩니다:

- **상태 집합 \(S\):** 환경의 가능한 모든 상태를 나타냅니다.
- **행동 집합 \(A\):** 에이전트가 선택할 수 있는 가능한 모든 행동을 나타냅니다.
- **전이 확률 \(P\):** 상태 \(s\)에서 행동 \(a\)를 취했을 때, 상태 \(s'\)로 전이될 확률 \(P(s'|s, a)\)를 나타냅니다.
- **보상 함수 \(R\):** 상태 \(s\)에서 행동 \(a\)를 취했을 때, 즉시 받는 보상 \(R(s, a)\)를 정의합니다.

### 2. **정책 (Policy)**

정책 \(\pi\)는 주어진 상태에서 행동을 선택하는 전략입니다. 이는 상태에서 행동을 확률적으로 결정하는 함수입니다:

- **확률적 정책:** \(\pi(a|s)\)는 상태 \(s\)에서 행동 \(a\)를 선택할 확률을 제공합니다.
- **결정적 정책:** \(\pi(s)\)는 상태 \(s\)에서 선택할 행동 \(a\)를 결정합니다.

### 3. **가치 함수 (Value Function)**

가치 함수는 상태나 상태-행동 쌍의 장기적인 가치를 평가합니다:

- **상태 가치 함수 \(V(s)\):** 상태 \(s\)에서 시작하여 최적 정책을 따를 때 기대되는 총 보상입니다. 수식으로 표현하면,

  $$
  V(s) = \mathbb{E}[R_t | S_t = s]
  $$

  여기서 \(R_t\)는 시간 \(t\)에서의 총 보상입니다.

- **행동 가치 함수 \(Q(s, a)\):** 상태 \(s\)에서 행동 \(a\)를 취한 후, 최적 정책을 따를 때 기대되는 총 보상입니다. 수식으로 표현하면,
  $$
  Q(s, a) = \mathbb{E}[R_t | S_t = s, A_t = a]
  $$

### 4. **벨만 방정식 (Bellman Equation)**

벨만 방정식은 가치 함수와 최적 정책의 관계를 정의합니다:

- **벨만 기대 방정식 (상태 가치 함수):**

  $$
  V(s) = \mathbb{E}_{a \sim \pi}[R(s, a) + \gamma \sum_{s'} P(s'|s, a) V(s')]
  $$

- **벨만 기대 방정식 (행동 가치 함수):**
  $$
  Q(s, a) = \mathbb{E}[R(s, a) + \gamma \sum_{s'} P(s'|s, a) \max_{a'} Q(s', a')]
  $$

여기서 \(\gamma\)는 할인 인자(discount factor)로, 미래의 보상에 대한 현재의 가치를 조정합니다.

### 5. **강화학습 알고리즘**

강화학습에는 여러 알고리즘이 있으며, 대표적으로는 다음과 같습니다:

- **정책 경량화 방법 (Policy Gradient Methods):** 정책을 직접 파라미터화하고 경량한 최적화 알고리즘을 사용하여 정책의 성능을 개선합니다. 대표적으로 REINFORCE 알고리즘이 있습니다.
- **Q-러닝 (Q-Learning):** 모델이 없는 오프라인 학습 방법으로, 에이전트가 환경과 상호작용하여 Q-값을 업데이트합니다.
- **SARSA (State-Action-Reward-State-Action):** Q-러닝과 유사하지만, 실제로 취한 행동을 기반으로 Q-값을 업데이트합니다.
- **딥 Q-네트워크 (Deep Q-Network, DQN):** Q-러닝을 딥러닝과 결합하여 복잡한 환경에서 학습할 수 있도록 합니다.

### 6. **핵심 수학적 도구**

- **몬테 카를로 방법 (Monte Carlo Methods):** 보상 시퀀스의 평균을 계산하여 가치 함수를 추정합니다.
- **템포랄 디퍼런스 학습 (Temporal Difference Learning, TD Learning):** 현재 추정 값을 기반으로 가치 함수를 점진적으로 업데이트합니다.
- **함수 근사 (Function Approximation):** 상태나 행동의 가치를 계산하기 위해 신경망 등의 함수 근사기를 사용합니다.

---

## 알고리즘 상세

### 1. **정책 기반 알고리즘 (Policy-Based Methods)**

#### **REINFORCE**

- **목표:** 전체 에피소드의 보상을 사용하여 정책을 업데이트합니다.

- **수식:**
  $$
  \nabla J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \nabla \log \pi_\theta(a_i | s_i) \cdot R_i
  $$
  - \(\pi\_\theta(a | s)\): 상태 \(s\)에서 행동 \(a\)를 선택할 확률을 나타내는 정책 함수입니다.
  - \(R_i\): 에피소드 \(i\)의 총 보상입니다.
  - \(\nabla \log \pi\_\theta(a_i | s_i)\): 정책의 파라미터 \(\theta\)에 대한 기울기입니다. 이는 정책이 주어진 상태에서 행동을 선택할 확률의 변화를 나타냅니다.
  - **설명:** 이 수식은 전체 에피소드에서 얻은 보상을 기반으로 정책의 파라미터를 업데이트합니다. 높은 보상을 받은 행동의 확률을 증가시키는 방향으로 정책을 조정합니다.

#### **Actor-Critic**

- **목표:** 두 개의 모델을 사용하여 정책 (Actor)과 가치 함수 (Critic)를 학습합니다.

- **수식:**
  $$
  \Delta \theta \propto \nabla \log \pi_\theta(a_t | s_t) \cdot \delta_t
  $$
  - \(\delta*t = R_t + \gamma V(s*{t+1}) - V(s_t)\): 템포랄 디퍼런스(TD) 오차입니다.
  - **설명:** Actor는 Critic이 계산한 TD 오차 \(\delta_t\)를 사용하여 정책의 기울기를 계산하고, 이를 기반으로 정책을 업데이트합니다. TD 오차는 예측한 가치와 실제 보상 간의 차이를 나타냅니다.

### 2. **가치 기반 알고리즘 (Value-Based Methods)**

#### **Q-러닝 (Q-Learning)**

- **목표:** 상태-행동 쌍의 Q-값을 업데이트하여 최적의 정책을 학습합니다.

- **수식:**
  $$
  Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
  $$
  - \(\alpha\): 학습률로, 업데이트의 크기를 조정합니다.
  - \(r\): 현재 상태 \(s\)와 행동 \(a\)에서 받은 즉시 보상입니다.
  - \(\gamma\): 할인 인자, 미래 보상의 현재 가치에 대한 가중치입니다.
  - \(\max\_{a'} Q(s', a')\): 다음 상태 \(s'\)에서 가능한 행동 중 최대 Q-값입니다.
  - **설명:** 이 수식은 Q-값을 업데이트하여 현재 상태에서 행동을 취한 결과로 기대되는 보상을 계산합니다. 학습률 \(\alpha\)는 업데이트의 비율을 조정합니다.

#### **SARSA (State-Action-Reward-State-Action)**

- **목표:** 실제로 선택한 행동을 기반으로 Q-값을 업데이트합니다.

- **수식:**
  $$
  Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]
  $$
  - \(a'\): 다음 상태 \(s'\)에서 선택한 행동입니다.
  - **설명:** Q-러닝과 유사하지만, SARSA는 다음 상태에서 실제로 선택된 행동 \(a'\)을 사용하여 Q-값을 업데이트합니다. 이는 정책이 현재의 행동을 기반으로 학습되도록 합니다.

### 3. **모델 기반 알고리즘 (Model-Based Methods)**

#### **Dyna-Q**

- **목표:** 모델 기반 접근 방식을 사용하여 가상의 경험을 생성하고 Q-값을 업데이트합니다.

- **설명:** Dyna-Q는 Q-러닝과 모델 기반 접근을 결합하여 모델이 예측한 전이 확률과 보상을 사용해 가상의 경험을 생성합니다. 이 가상의 경험은 실제 경험과 함께 사용되어 Q-값을 업데이트합니다.

### 4. \*\*심층 강화학습 (Deep Rein

forcement Learning)\*\*

#### **Deep Q-Network (DQN)**

- **목표:** 신경망을 사용하여 Q-값을 근사합니다. 경험 리플레이와 타겟 네트워크를 사용하여 학습 안정성을 높입니다.

- **수식:**
  $$
  L(\theta) = \mathbb{E}_{s, a, r, s'} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta) \right)^2 \right]
  $$
  - \(Q(s, a; \theta)\): 신경망으로 근사한 Q-값입니다.
  - \(\theta\): 현재 신경망의 파라미터입니다.
  - \(\theta^-\): 타겟 네트워크의 파라미터로, 일정 주기로 업데이트됩니다.
  - **설명:** 이 수식은 Q-값의 차이를 제곱하여 손실 함수를 정의합니다. 신경망을 사용해 Q-값을 근사하고, 경험 리플레이와 타겟 네트워크를 통해 학습 안정성을 개선합니다.

#### **Advantage Actor-Critic (A2C) 및 Asynchronous Actor-Critic Agents (A3C)**

- **목표:** Actor-Critic 구조를 사용하여 정책과 가치 함수를 동시에 학습합니다.

- **수식:**
  $$
  \text{Actor Update: } \nabla \log \pi_\theta(a_t | s_t) \cdot \delta_t
  $$
  $$
  \text{Critic Update: } \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
  $$
  - \(\delta_t\): TD 오차로, Critic이 계산한 가치 추정의 차이입니다.
  - **설명:** Actor는 TD 오차를 사용하여 정책의 기울기를 업데이트하고, Critic은 TD 오차를 기반으로 가치 함수를 업데이트합니다. A3C는 여러 에이전트를 병렬로 학습시켜 학습 효율성을 높입니다.

#### **Proximal Policy Optimization (PPO)**

- **목표:** 정책 업데이트를 안정적으로 수행하기 위해 클리핑 기법을 사용합니다.

- **수식:**
  $$
  L^{\text{PPO}}(\theta) = \mathbb{E} \left[ \min \left( \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)} \hat{A}_t, \text{clip} \left( \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)}, 1 - \epsilon, 1 + \epsilon \right) \hat{A}_t \right) \right]
  $$
  - \(\pi\_\theta(a_t | s_t)\): 새로운 정책에서 상태 \(s_t\)에서 행동 \(a_t\)를 선택할 확률입니다.
  - \(\pi*{\theta*{\text{old}}}(a_t | s_t)\): 이전 정책에서 상태 \(s_t\)에서 행동 \(a_t\)를 선택할 확률입니다.
  - \(\hat{A}\_t\): 어드밴티지 추정치로, 행동이 평균보다 얼마나 더 좋은지를 나타냅니다.
  - \(\epsilon\): 클리핑 범위로, 정책 업데이트의 범위를 제한하여 안정성을 높입니다.
  - **설명:** PPO는 정책이 너무 많이 변경되는 것을 방지하기 위해, 새로운 정책과 이전 정책 간의 비율을 클리핑하여 안정적인 업데이트를 보장합니다.
