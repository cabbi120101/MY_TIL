# 6. 통계적 데이터 분석 (Advanced Statistical Data Analysis)

통계적 데이터 분석은 데이터를 수집, 요약, 해석하여 의미 있는 인사이트를 도출하는 과정입니다. 심화된 통계적 분석은 데이터의 특성을 깊이 이해하고, 가설을 검정하며, 예측 모델을 구축하고 평가하는 데 중요한 도구입니다. 다양한 통계적 기법을 통해 데이터를 다각도로 분석하고, 데이터 기반 의사결정을 지원합니다.

## 6.1 통계적 데이터 분석의 개요
- **정의:** 통계적 분석은 수집된 데이터를 통해 인사이트를 도출하고 의사결정을 지원하는 방법론입니다.
- **중요성:** 데이터의 특성을 이해하고, 불확실성을 관리하며, 예측과 의사결정을 과학적으로 뒷받침합니다.
- **적용 분야:** 비즈니스, 금융, 의료, 사회과학 등 다양한 분야에서 데이터 기반의 문제 해결에 활용됩니다.

## 6.2 기술 통계학 (Descriptive Statistics)
기술 통계는 데이터를 요약하고 설명하기 위한 기초적인 통계적 방법을 제공합니다. 심화된 기술 통계학에서는 데이터의 분포, 경향성, 비대칭성 등을 정밀하게 파악할 수 있습니다.

### 6.2.1 주요 개념
- **중앙 경향 (Central Tendency):** 평균, 중앙값, 최빈값 등 데이터의 중심 위치를 나타내는 값으로 데이터의 대표값을 결정합니다.
- **산포도 (Dispersion):** 분산, 표준편차, 범위 등 데이터가 중심에서 얼마나 퍼져 있는지를 나타내어 데이터의 변동성을 설명합니다.
- **왜도와 첨도 (Skewness and Kurtosis):** 데이터의 비대칭성과 분포의 뾰족함을 설명하는 지표로, 데이터의 분포 특성을 심층적으로 이해할 수 있습니다.

### 6.2.2 사용 상황과 예시
- **사용 상황:** 데이터의 기본적인 특성을 이해하고, 데이터의 분포와 변동성을 요약할 때 사용됩니다. 분석 초기 단계에서 데이터의 전반적인 구조를 파악하는 데 유용합니다.
- **예시:** 회사의 직원 연봉 데이터를 분석하여 평균 연봉, 최저/최고 연봉, 연봉 분포의 왜도와 첨도를 평가하여 연봉 정책 개선에 활용.
- **사용 도구:** Pandas의 `describe()`, NumPy, SciPy의 `stats.describe()`

## 6.3 가설 검정 (Hypothesis Testing)
가설 검정은 데이터가 특정한 가설을 지지하는지 평가하는 통계적 방법으로, 연구나 실험에서 데이터에 기반한 결론을 도출하는 데 필수적입니다.

### 6.3.1 주요 개념
- **귀무가설 (Null Hypothesis, H0):** 연구에서 입증하고자 하는 주장이 없을 때 설정하는 기본 가설로, 검정의 출발점입니다.
- **대립가설 (Alternative Hypothesis, H1):** 귀무가설에 반대되는 주장으로, 연구자가 입증하고자 하는 가설입니다.
- **유의수준 (Significance Level, α):** 가설 검정에서 오류를 허용하는 수준으로, 일반적으로 0.05나 0.01을 사용합니다.
- **p-값 (p-value):** 귀무가설이 참이라는 가정하에 관찰된 데이터가 발생할 확률을 나타내며, p-값이 작을수록 대립가설을 지지합니다.

### 6.3.2 주요 가설 검정 기법
- **t-검정 (t-Test):** 두 집단의 평균을 비교하여 통계적 차이가 있는지 평가합니다.
  - **적합 상황:** 두 그룹 간의 평균 차이를 비교할 때 사용되며, 독립 표본 t-검정과 대응 표본 t-검정으로 나뉩니다.
  - **예시:** 신약 치료군과 대조군의 평균 혈압 비교를 통해 신약의 효과 평가.
  - **도구:** SciPy의 `ttest_ind()`, `ttest_rel()`

- **카이제곱 검정 (Chi-Square Test):** 범주형 데이터에서 관측 빈도와 기대 빈도의 차이를 평가하여 변수 간의 독립성을 확인합니다.
  - **적합 상황:** 두 범주형 변수 간의 연관성을 평가할 때 사용합니다.
  - **예시:** 성별에 따른 구매 선호도 차이를 분석하여 마케팅 전략 수립.
  - **도구:** SciPy의 `chi2_contingency()`

- **ANOVA (Analysis of Variance):** 세 개 이상의 그룹 간 평균 차이를 비교하여 그룹 간 변동을 평가합니다.
  - **적합 상황:** 세 개 이상의 그룹 평균이 통계적으로 다른지 평가할 때 사용합니다.
  - **예시:** 다양한 교육 프로그램 간 학업 성취도 비교를 통해 프로그램 효과 분석.
  - **도구:** SciPy의 `f_oneway()`

### 6.3.3 가설 검정 절차
1. **가설 설정:** 귀무가설(H0)과 대립가설(H1)을 설정합니다.
2. **유의수준 설정:** 보통 0.05 또는 0.01의 유의수준을 설정합니다.
3. **검정 통계량 계산:** t-검정, 카이제곱 검정 등 적절한 통계량을 계산합니다.
4. **p-값 비교:** p-값이 유의수준보다 작으면 귀무가설을 기각하고, 대립가설을 채택합니다.

## 6.4 회귀 분석 (Regression Analysis)
회귀 분석은 변수 간의 관계를 모델링하고, 특정 변수의 값을 예측하는 데 사용되는 기법으로, 통계적 예측의 핵심 도구입니다.

### 6.4.1 주요 개념
- **선형 회귀 (Linear Regression):** 독립 변수와 종속 변수 간의 선형 관계를 모델링하여 예측합니다.
- **로지스틱 회귀 (Logistic Regression):** 종속 변수가 범주형일 때 사용되는 회귀 분석으로, 사건이 발생할 확률을 모델링합니다.
- **다중 회귀 (Multiple Regression):** 두 개 이상의 독립 변수를 사용하여 종속 변수를 예측하며, 상호작용 효과도 고려할 수 있습니다.

### 6.4.2 사용 상황과 예시
- **선형 회귀:** 주택 가격을 면적과 방 개수에 따라 예측하여 시장 동향 파악.
- **로지스틱 회귀:** 고객의 이탈 여부를 예측하여 이탈 방지 전략 수립.
- **다중 회귀:** 자동차의 연비를 엔진 크기, 차량 무게, 제조사 등을 이용해 예측하여 차량 성능 개선에 활용.
- **사용 도구:** Statsmodels, Scikit-learn

### 6.4.3 회귀 분석 절차
1. **데이터 준비:** 독립 변수(X)와 종속 변수(Y)를 정의합니다.
2. **모델 생성:** 회귀 모델을 생성하고, 데이터를 학습합니다.
3. **모델 평가:** R², RMSE, AIC 등의 지표를 사용하여 모델의 성능을 평가합니다.
4. **결과 해석:** 회귀 계수와 p-값을 통해 독립 변수의 영향력을 평가하고, 실무 적용 방안을 모색합니다.

## 6.5 분류 분석 (Classification Analysis)
분류 분석은 데이터를 미리 정의된 카테고리나 클래스로 나누는 기법으로, 예측 모델 구축에 널리 사용됩니다.

### 6.5.1 주요 기법
- **로지스틱 회귀 (Logistic Regression):** 이진 분류 문제에 사용되며, 사건이 발생할 확률을 모델링합니다.
- **서포트 벡터 머신 (Support Vector Machine):** 데이터의 경계를 학습하여 분류하는 기법으로 고차원 데이터에 유용합니다.
- **결정 트리 (Decision Tree):** 규칙 기반의 모델로, 분류 및 예측에 사용되며 이해하기 쉽습니다.

### 6.5.2 사용 상황과 예시
- **이메일 스팸 필터링:** 이메일을 스팸과 정상 메일로 분류하여 사용자 경험 개선.
- **의료 진단:** 환자의 증상으로 질병을 분류하여 진단 정확도 향상.
- **고객 분류:** 고객 데이터를 사용하여 구매 가능성 있는 고객군 분류하여 마케팅 전략 최적화.
- **사용 도구:** Scikit-learn, TensorFlow, Keras

## 6.6 군집 분석 (Clustering Analysis)
군집 분석은 유사한 특성을 가진 데이터 포인트를 그룹으로 묶는 비지도 학습 기법으로, 데이터 탐색과 세분화에 효과적입니다.

### 6.6.1 주요 기법
- **K-평균 (K-Means):** 데이터를 K개의 클러스터로 묶고, 각 클러스터의 중심점을 계산하여 데이터 포인트를 재배치합니다.
- **계층적 군집 분석 (Hierarchical Clustering):** 데이터 포인트를 상향식 또는 하향식으로 계층 구조로 묶어 군집을 형성합니다.
- **DBSCAN (Density-Based Spatial Clustering):** 밀도 기반의 군집 분석으로, 노이즈와 이상치를 처리하는 데 효과적입니다.

### 6.6.2 사용 상황과 예시
- **고객 세분화:** 고객의 행동 패턴에 따라 그룹화하여 맞춤형 마케팅 전략 수립 및 고객 관리 최적화.
- **이미지 분할:** 이미지 내 유사한 픽셀을 묶어 영역 분할하여 이미지 분석과 객체 인식에 활용.
- **이상치 탐지:** 비정상적인 행동이나 이상 데이터를 클러스터 밖으로 분류하여 리스크 관리에 활용.
- **사용 도구:** Scikit-learn, SciPy, HDBSCAN

## 6.7 통계적 모델 평가 및 검증 (Model Evaluation and Validation)
모델의 성능을 평가하고 검증하는 것은 통계적 분석에서 필수적인 단계로, 모델의 신뢰성과 예측력을 확인합니다.

### 6.7.1 주요 지표
- **R² (Coefficient of Determination):** 회귀 모델이 데이터를 얼마나 잘 설명하는지 측정합니다. 값이 1에 가까울수록 모델의 설명력이 높습니다.
- **RMSE (Root Mean Squared Error):** 예측 값과 실제 값의 차이를 제곱하여 평균한 후 루트를 씌운 값으로, 예측 오차의 크기를 나타냅니다.
- **정확도 (Accuracy), 정밀도 (Precision), 재현율 (Recall):** 분류 모델의 성능을 평가하는 지표들로, 각각 분류의 정확성과 분류 오류의 비율을 측정합니다.

### 6.7.2 검증 방법
- **교차 검증 (Cross-Validation):** 데이터를 여러 번 분할하여 모델을 평가하여 일반화 성능을 측정하고, 과적합을 방지합니다.
- **훈련/검증/테스트 분할 (Train/Validation/Test Split):** 데이터를 훈련, 검증, 테스트로 나누어 모델 성능을 확인하며, 최적의 모델 파라미터를 찾는 데 활용합니다.

---

이 섹션을 통해 통계적 데이터 분석의 다양한 기법과 그 적용 방법을 깊이 이해할 수 있습니다. 각 기법의 사용 상황과 실제 예시를 통해 데이터 분석의 기본과 심화된 기법을 학습하고, 실전에서 적용할 수 있는 능력을 키워보세요!
